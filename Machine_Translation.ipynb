{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞\n"
      ],
      "metadata": {
        "id": "iDt3mCrLVUBU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GM8HnqG7dG6I",
        "outputId": "c9b9d79e-b9b0-4dfb-ee58-e03011006e94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/gdrive/My Drive/DL/HW2/DL_HW2_chkpt.zip' /content/"
      ],
      "metadata": {
        "id": "Hf8X6_6XLgUp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip *.zip"
      ],
      "metadata": {
        "id": "31FTgYk8MAGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r DL_HW2_chkpt/requirements.txt"
      ],
      "metadata": {
        "id": "Y_GrX40tMK04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! cp -R DL_HW2_chkpt/* /content/"
      ],
      "metadata": {
        "id": "aaOdDXTTMmkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp '/content/gdrive/My Drive/DL/HW2/chkpt.zip' /content/"
      ],
      "metadata": {
        "id": "hBUc-o9zZVgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checkpoint"
      ],
      "metadata": {
        "id": "gTNxYWHGPUUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python train_model.py --data-dir \"processed_data/\" --tokenizer-path \"tokenizer/\" --num-epochs 5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjgOQG6NM0VQ",
        "outputId": "7c9423f6-1310-4969-8443-1bf233db7dac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjuliana_win\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20221206_032022-36skbqbd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33maaa\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/juliana_win/Machine%20translation%20%28DL%29\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/juliana_win/Machine%20translation%20%28DL%29/runs/36skbqbd\u001b[0m\n",
            "  0% 0/5 [00:00<?, ?it/s]0 \t 0.12900066375732422\n",
            "500 \t 117.19082546234131\n",
            "1000 \t 235.34109830856323\n",
            "1500 \t 351.7420370578766\n",
            "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py:276: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:175.)\n",
            "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:1679: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Epoch: 1\n",
            "Train loss: 2.9383653205744227\t Val loss: 1.5026605129241943\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            " 20% 1/5 [06:18<25:14, 378.74s/it]0 \t 0.1265273094177246\n",
            "500 \t 120.706871509552\n",
            "1000 \t 236.30419254302979\n",
            "1500 \t 356.5198583602905\n",
            "Epoch: 2\n",
            "Train loss: 1.312705340777865\t Val loss: 1.3925612228257316\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            " 40% 2/5 [12:41<19:03, 381.21s/it]0 \t 0.11847376823425293\n",
            "500 \t 118.70959424972534\n",
            "1000 \t 234.36710000038147\n",
            "1500 \t 355.20539140701294\n",
            "Epoch: 3\n",
            "Train loss: 1.1944381612058013\t Val loss: 1.2202169043677193\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            " 60% 3/5 [19:03<12:43, 381.59s/it]0 \t 0.11374068260192871\n",
            "500 \t 119.10429072380066\n",
            "1000 \t 233.90353798866272\n",
            "1500 \t 352.91211438179016\n",
            "Epoch: 4\n",
            "Train loss: 1.082922701509843\t Val loss: 1.1368467467171806\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            " 80% 4/5 [25:25<06:21, 381.70s/it]0 \t 0.1216878890991211\n",
            "500 \t 116.94513416290283\n",
            "1000 \t 231.28978824615479\n",
            "1500 \t 353.475594997406\n",
            "Epoch: 5\n",
            "Train loss: 1.0291236448732222\t Val loss: 1.1267964669636317\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "100% 5/5 [31:46<00:00, 381.30s/it]\n",
            "['So , a few years ago , here , here \\' s a book called \" A - called a design picture of the story of the universe , \" The idea', 'The idea is really just a very long - term , very , the way that the animal can be able to be able to take the structure of a map , and a half , a half - foot - up', 'The left is to be just the end of the top of the', \"And because it ' s really easy , it ' s really easy because it ' s really hard to be very difficult to be very difficult to\", \"And so I thought that this idea is a interesting idea , and I ' ve been a design of design with a computer .\", 'It was a huge amount of hard - based - to -', \"And so , I ' ve been in about about 20 - year - year - year - old , and I ' ve got to talk about , and in the world , and there ' s a new way that I ' m going to do with this , and I want to talk about this ,\"]\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "\n",
            "BLEU with greedy search: 5.558360786413738\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33maaa\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/juliana_win/Machine%20translation%20%28DL%29/runs/36skbqbd\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221206_032022-36skbqbd/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/*.py '/content/gdrive/My Drive/DL_HW2_chkpt/'\n",
        "!cp -R /content/data/ '/content/gdrive/My Drive/DL_HW2_chkpt/'\n",
        "!cp -R /content/processed_data/ '/content/gdrive/My Drive/DL_HW2_chkpt/'\n",
        "!cp -R /content/tokenizer/ '/content/gdrive/My Drive/DL_HW2_chkpt/'\n",
        "!cp -R /content/*txt '/content/gdrive/My Drive/DL_HW2_chkpt/'\n",
        "!cp -R /content/*best.pth '/content/gdrive/My Drive/DL_HW2_chkpt/'\n",
        "!cp -R /content/*best.pth '/content/gdrive/My Drive/DL_HW2_chkpt/'"
      ],
      "metadata": {
        "id": "B50yF-1FNzCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –ó–∞–ø—É—Å–∫ –Ω–∞ 30 —ç–ø–æ—Ö"
      ],
      "metadata": {
        "id": "dBNOU2FndJ8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! python train_model.py --data-dir \"processed_data/\" --tokenizer-path \"tokenizer/\" --num-epochs 30"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhNG4n2RcFWA",
        "outputId": "f5ff8df9-d7f4-4f23-d169-39c74535423d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjuliana_win\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.13.6 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.5\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20221208_072740-1d1thmqt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33maaa\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/juliana_win/Machine%20translation%20%28DL%29\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/juliana_win/Machine%20translation%20%28DL%29/runs/1d1thmqt\u001b[0m\n",
            "  0% 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/torch/nn/modules/transformer.py:276: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:175.)\n",
            "  output = torch._nested_tensor_from_mask(output, src_key_padding_mask.logical_not(), mask_check=False)\n",
            "/usr/local/lib/python3.8/dist-packages/torch/optim/lr_scheduler.py:1679: UserWarning: To get the last learning rate computed by the scheduler, please use `get_last_lr()`.\n",
            "  warnings.warn(\"To get the last learning rate computed by the scheduler, \"\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 1\n",
            "Train loss: 4.9963524195718465\t Val loss: 1.8405706201280867\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 2\n",
            "Train loss: 1.5029551576383366\t Val loss: 1.5078253575733729\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 3\n",
            "Train loss: 1.3215371371796412\t Val loss: 1.3686207107135229\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 4\n",
            "Train loss: 1.1676546279688058\t Val loss: 1.1462688105446952\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 5\n",
            "Train loss: 0.9967339572699173\t Val loss: 0.9835625972066607\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 6\n",
            "Train loss: 0.8836490392684937\t Val loss: 0.9006574749946594\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 7\n",
            "Train loss: 0.8157987742320351\t Val loss: 0.8413684708731515\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 8\n",
            "Train loss: 0.7669348032022856\t Val loss: 0.8124873297555106\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 9\n",
            "Train loss: 0.7363729785873283\t Val loss: 0.7971620431968144\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 10\n",
            "Train loss: 0.7120616395095861\t Val loss: 0.7674083709716797\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 11\n",
            "Train loss: 0.6892917632501318\t Val loss: 0.7624521425792149\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 12\n",
            "Train loss: 0.6759230071898573\t Val loss: 0.7542814910411835\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 13\n",
            "Train loss: 0.6606745264730098\t Val loss: 0.7427312902041844\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 14\n",
            "Train loss: 0.6514055578234773\t Val loss: 0.7376521144594465\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 15\n",
            "Train loss: 0.6392984157393438\t Val loss: 0.7495744398662022\n",
            "\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 16\n",
            "Train loss: 0.6244625164855341\t Val loss: 0.7245349287986755\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 17\n",
            "Train loss: 0.6145734621871333\t Val loss: 0.7196216583251953\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 18\n",
            "Train loss: 0.6007748579201491\t Val loss: 0.7186310674463\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 19\n",
            "Train loss: 0.5964869156757497\t Val loss: 0.7153226733207703\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 20\n",
            "Train loss: 0.5816892122815115\t Val loss: 0.7129372571195874\n",
            "\n",
            "New best loss! Saving checkpoint\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 21\n",
            "Train loss: 0.5757324302788847\t Val loss: 0.713132266487394\n",
            "\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 22\n",
            "Train loss: 0.5671863201427163\t Val loss: 0.7140391043254307\n",
            "\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 23\n",
            "Train loss: 0.5582420001674143\t Val loss: 0.7179364391735622\n",
            "\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 24\n",
            "Train loss: 0.5488235566186609\t Val loss: 0.7189327520983559\n",
            "\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 25\n",
            "Train loss: 0.5438735887500811\t Val loss: 0.7175456711224147\n",
            "\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 26\n",
            "Train loss: 0.5381249510538504\t Val loss: 0.71981509242739\n",
            "\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 27\n",
            "Train loss: 0.533063547063318\t Val loss: 0.7216604437146869\n",
            "\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 28\n",
            "Train loss: 0.5279982728432424\t Val loss: 0.7244565061160496\n",
            "\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 29\n",
            "Train loss: 0.5255451945969777\t Val loss: 0.722668217761176\n",
            "\n",
            "Figure(640x480)\n",
            "Figure(640x480)\n",
            "Epoch: 30\n",
            "Train loss: 0.5249169470915883\t Val loss: 0.7229151598044804\n",
            "\n",
            "100% 30/30 [3:14:29<00:00, 388.98s/it]\n",
            "['Some years ago , here at TED , Peter S ill , asked a design competition called \" The marshmallow \" the marshmallow .\"', 'The idea is pretty simple .', 'The marshmallow has to be on top of it .', \"And , although it seems really , it ' s really hard because people are really hard to work very quickly .\", 'And so I thought this is an interesting idea , and I turned it into a design workshop .', 'It was a huge success .', \"And since then , I ' ve been doing about 70 workshops worldwide , with students and architects and architects , at the Fortune 50 company , and there ' s something about this task , which allows us to do deep sensations in nature , and I want to share some of you with you .\"]\n",
            "That's 100 lines that end in a tokenized period ('.')\n",
            "It looks like you forgot to detokenize your test data, which may hurt your score.\n",
            "If you insist your data is detokenized, or don't care, you can suppress this message with the `force` parameter.\n",
            "\n",
            "BLEU with greedy search: 20.512498787641128\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33maaa\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/juliana_win/Machine%20translation%20%28DL%29/runs/1d1thmqt\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 3 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20221208_072740-1d1thmqt/logs\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/checkpoint_best.pth '/content/gdrive/My Drive/DL/HW2/'\n",
        "!cp /content/checkpoint_last.pth '/content/gdrive/My Drive/DL/HW2/'\n",
        "!cp /content/answers_greedy.txt '/content/gdrive/My Drive/DL/HW2/'"
      ],
      "metadata": {
        "id": "mc_5tqPikw2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam search"
      ],
      "metadata": {
        "id": "LYO05U5LPu4-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# –°–∫–∞—á–∞–µ–º –º–æ–¥–µ–ª—å –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ä–∞–∑–¥–µ–ª–∞\n",
        "!cp '/content/gdrive/My Drive/DL/HW2/chkpt.zip' /content/\n",
        "\n",
        "! unzip chkpt.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VURo14SZhFs",
        "outputId": "db64222f-da03-4dbb-9d36-10ad70af7b10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  chkpt.zip\n",
            "   creating: chkpt/\n",
            "  inflating: chkpt/checkpoint_best_epoch_20.pth  \n",
            "  inflating: __MACOSX/chkpt/._checkpoint_best_epoch_20.pth  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –ó–∞–ø—É—Å—Ç–∏–º —Å –±–∏–º—Å–∞–π–∑ 1 –∏ —É–¥–æ—Å—Ç–æ–≤–µ—Ä–∏–º—Å—è —á—Ç–æ –æ—Ç–≤–µ—Ç—ã –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –∏ –±–ª—é –ø–æ—Ö–æ–∂–∏–µ\n",
        "\n",
        "! python train_model.py --data-dir \"processed_data/\" --tokenizer-path \"tokenizer/\" --num-epochs 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KbjGKTXaFgM",
        "outputId": "f8ed71fb-562e-4332-d7a3-39fb8ccf289c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0/1 [00:00<?, ?it/s]\r  0% 0/1 [00:00<?, ?it/s]\n",
            "\n",
            "BLEU with greedy search: 21.14819696852553\n",
            "BLEU with beam search: 21.16278779202183\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# –ó–∞–ø—É—Å—Ç–∏–º —Å –±–∏–º —Å–∞–π–∑ 5\n",
        "\n",
        "! python train_model.py --data-dir \"processed_data/\" --tokenizer-path \"tokenizer/\" --num-epochs 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L8K1_RSTSTVx",
        "outputId": "9139eb70-c413-4f9e-c5ac-9811db024559"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r  0% 0/1 [00:00<?, ?it/s]\r  0% 0/1 [00:00<?, ?it/s]\n",
            "\n",
            "BLEU with greedy search: 21.14819696852553\n",
            "BLEU with beam search: 23.18495674381781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "–ú–æ–∂–µ–º –∑–∞–º–µ—Ç–∏—Ç—å, —á—Ç–æ –∫–∞–∫ –∏ –æ–∂–∏–¥–∞–ª–æ—Å—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º beam_search –¥–∞–ª–æ –∫–∞—á–µ—Å—Ç–≤–æ –ª—É—á—à–µ. –Æ—Ö—É"
      ],
      "metadata": {
        "id": "IeNsT06X6_oR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam search batched"
      ],
      "metadata": {
        "id": "IbAakUcCP9F_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "–Ø –ø—ã—Ç–∞–ª–∞—Å—å, –Ω–æ —ç—Ç–æ –∫–∞–∫–∞—è-—Ç–æ –∂—É—Ç–∫–∞—è –∂—É—Ç—å, –Ω–µ –ø–æ–Ω–∏–º–∞–µ–º–æ, –ø–æ—ç—Ç–æ–º—É –ª—É—á—à–µ –≤–º–µ—Å—Ç–æ –ø—Ä–æ–≤–µ—Ä–∫–∏ —ç—Ç–æ–≥–æ –∫–æ–¥–∞ –ø–æ—Å–º–æ—Ç—Ä–∏ –º–µ–º—ã:\n",
        "\n",
        "–†–∞–∑: https://drive.google.com/file/d/1SjCPGgbfFaMSondFeS2QrV-eF7z5qih4/view?usp=sharing\n",
        "\n",
        "–î–≤–∞: https://drive.google.com/file/d/1QVXg0jmtntPxKz95gVMKSNGSzYgxqe_U/view?usp=sharing"
      ],
      "metadata": {
        "id": "XepLmf2GP__b"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tQRiHB94R8Y8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# –°–æ–±–∏—Ä–∞–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π –∞—Ä—Ö–∏–≤"
      ],
      "metadata": {
        "id": "P8vIVHl5R8qn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir '/content/DL_HW_2_big'"
      ],
      "metadata": {
        "id": "UqLhQIN8SAF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv tokenizer/ /content/DL_HW_2_big\n",
        "!mv data/ /content/DL_HW_2_big\n",
        "!mv processed_data/ /content/DL_HW_2_big\n",
        "\n",
        "!mv *.py /content/DL_HW_2_big\n",
        "\n",
        "!mv *.txt /content/DL_HW_2_big\n",
        "\n",
        "!mv chkpt.zip /content/DL_HW_2_big\n",
        "\n",
        "!mv *.md /content/DL_HW_2_big"
      ],
      "metadata": {
        "id": "SK6RmzY4SF_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!zip -r DL_HW_2_big.zip DL_HW_2_big/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4RZhrnxcUg1-",
        "outputId": "0b3be9ca-43a8-4592-91d3-015aff6c621e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: DL_HW_2_big/ (stored 0%)\n",
            "  adding: DL_HW_2_big/data/ (stored 0%)\n",
            "  adding: DL_HW_2_big/data/IWSLT17.TED.dev2010.de-en.de.xml (deflated 66%)\n",
            "  adding: DL_HW_2_big/data/IWSLT17.TED.tst2010.de-en.en.xml (deflated 68%)\n",
            "  adding: DL_HW_2_big/data/IWSLT17.TED.tst2010.de-en.de.xml (deflated 67%)\n",
            "  adding: DL_HW_2_big/data/README (deflated 59%)\n",
            "  adding: DL_HW_2_big/data/.DS_Store (deflated 94%)\n",
            "  adding: DL_HW_2_big/data/IWSLT17.TED.dev2010.de-en.en.xml (deflated 66%)\n",
            "  adding: DL_HW_2_big/data/README.md (deflated 56%)\n",
            "  adding: DL_HW_2_big/data/train.tags.de-en.en (deflated 65%)\n",
            "  adding: DL_HW_2_big/data/train.tags.de-en.de (deflated 64%)\n",
            "  adding: DL_HW_2_big/requirements.txt (deflated 31%)\n",
            "  adding: DL_HW_2_big/process_data.py (deflated 59%)\n",
            "  adding: DL_HW_2_big/answers_beam.txt (deflated 65%)\n",
            "  adding: DL_HW_2_big/decoding.py (deflated 76%)\n",
            "  adding: DL_HW_2_big/model.py (deflated 70%)\n",
            "  adding: DL_HW_2_big/answers_greedy.txt (deflated 65%)\n",
            "  adding: DL_HW_2_big/chkpt.zip (stored 0%)\n",
            "  adding: DL_HW_2_big/README.md (deflated 56%)\n",
            "  adding: DL_HW_2_big/tokenizer/ (stored 0%)\n",
            "  adding: DL_HW_2_big/tokenizer/tokenizer_de.json (deflated 71%)\n",
            "  adding: DL_HW_2_big/tokenizer/tokenizer_en.json (deflated 71%)\n",
            "  adding: DL_HW_2_big/train_model.py (deflated 72%)\n",
            "  adding: DL_HW_2_big/processed_data/ (stored 0%)\n",
            "  adding: DL_HW_2_big/processed_data/train.en.txt (deflated 64%)\n",
            "  adding: DL_HW_2_big/processed_data/val.en.txt (deflated 62%)\n",
            "  adding: DL_HW_2_big/processed_data/val.de.txt (deflated 62%)\n",
            "  adding: DL_HW_2_big/processed_data/train.de.txt (deflated 63%)\n",
            "  adding: DL_HW_2_big/processed_data/test.en.txt (deflated 63%)\n",
            "  adding: DL_HW_2_big/processed_data/test.de.txt (deflated 63%)\n",
            "  adding: DL_HW_2_big/data.py (deflated 72%)\n",
            "  adding: DL_HW_2_big/.ipynb_checkpoints/ (stored 0%)\n"
          ]
        }
      ]
    }
  ]
}